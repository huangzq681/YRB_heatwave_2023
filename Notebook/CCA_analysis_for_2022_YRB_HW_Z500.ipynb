{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import proplot as pplot\n",
    "import matplotlib.pyplot as plt\n",
    "import cmaps\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from pyproj import transform\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import rioxarray\n",
    "import geopandas\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/zeqinhuang/Documents/paper/HW_track')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalyses_dir = {\n",
    "    'era5':'/Volumes/Seagate_HZQ/reanalyses/era5/',\n",
    "    'jra55':'/Volumes/Seagate_HZQ/reanalyses/jra55/'\n",
    "}\n",
    "\n",
    "datasets = list(reanalyses_dir.keys())\n",
    "\n",
    "domains = {\n",
    "    'D1':[80,160,10,50], #(80° to 160°E, 10° to 50° N)\n",
    "    'D2':[90,150,15,45], #(90° to 150°E, 15° to 45° N)\n",
    "    'D3':[100,140,20,40], #(100° to 140°E, 20° to 40° N)\n",
    "}\n",
    "\n",
    "domain_name = ['D1','D2','D3']\n",
    "\n",
    "target_griddes = {\n",
    "    'D1':{'lat': np.arange(10, 51, 1),'lon':np.arange(80, 161, 1)},\n",
    "    'D2':{'lat': np.arange(15, 46, 1),'lon':np.arange(90, 151, 1)},\n",
    "    'D3':{'lat': np.arange(20, 41, 1),'lon':np.arange(100, 141, 1)}\n",
    "}\n",
    "\n",
    "dist_funcs = ['E_Dist','P_Corr','S_Corr','Cosine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_domain(dataarray,dom):\n",
    "    try:\n",
    "        dataarray = dataarray.rename({'longitude':'lon','latitude':'lat'}) # for era5\n",
    "    except:\n",
    "        pass\n",
    "    lon_min = domains[dom][0]\n",
    "    lon_max = domains[dom][1]\n",
    "    lat_min = domains[dom][2]\n",
    "    lat_max = domains[dom][3]\n",
    "    data_dom = dataarray.sel(lat = slice(lat_max,lat_min),lon = slice(lon_min,lon_max))\n",
    "    return data_dom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-pass the Z500 and T2m data to create an un-skewed distribution of CCAs.\n",
    "This operation is to avoid falsely give the impression that a particular circluation pattern induced warmer conditions (Smoliak et al. 2015).\n",
    "Because surface air temperature (SAT) can be problematic to use CCAs from a climate significantly warmer or colder than the climate that one is trying to reconstruct.\n",
    "\n",
    "A published study has mentioned that different methods to detrend or high-pass the raw anthropogenic data generally produced similar results, thus it does not matter critically for\n",
    "the study here which detrending method is chosen. (See Lehner, Flavio, Clara Deser, Isla R. Simpson, and Laurent Terray. \"Attributing the US Southwest's recent shift into drier conditions.\" Geophysical Research Letters 45, no. 12 (2018): 6251-6261.)\n",
    "\n",
    "The high-pass procedure and code applied here are adapted from https://github.com/russellhz/extreme_heat_CCA. (also see: Horowitz, Russell L., Karen A. McKinnon, and Isla R. Simpson. “Circulation and Soil Moisture Contributions to Heatwaves in the United States.” Journal of Climate 35, no. 24 (December 15, 2022): 4431–48. https://doi.org/10.1175/JCLI-D-21-0156.1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier(period, k, n):\n",
    "    # Get periods for each fourier\n",
    "    p = [x/period for x in range(1,k+1)]\n",
    "    # Create array of ones\n",
    "    result = np.ones( (n, 2*k+1) )\n",
    "    # Fill in array with fourier series\n",
    "    for i in range(k):\n",
    "        result[:,(2*i)] = [np.sin(2*np.pi*x*p[i]) for x in range(1,n+1)]\n",
    "        result[:,(2*i+1)] = [np.cos(2*np.pi*x*p[i]) for x in range(1,n+1)]\n",
    "    return result\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def seasonality_removal_worker(data, I, X):    \n",
    "    # Regress fourier series on data\n",
    "    B = I.dot(data)\n",
    "    y_pred = X.dot(B)\n",
    "    return data - y_pred\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def seasonality_removal(data, k):\n",
    "    n=len(data)\n",
    "    # Create fourier series to regress\n",
    "    fX = fourier(365, k=k, n=n)\n",
    "    B = np.linalg.inv(fX.T.dot(fX)).dot(fX.T)\n",
    "    # Stack data and apply worker function to each gridcell\n",
    "    data_stack = data.stack(gridcell=('lat', 'lon'))\n",
    "    result = data_stack.groupby('gridcell').map(seasonality_removal_worker, I = B, X = fX).unstack('gridcell')\n",
    "    return result\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def seasonality_removal_vals(data, k):\n",
    "    n=len(data)\n",
    "    # Create fourier series to regress\n",
    "    fX = fourier(365, k=k, n=n)\n",
    "    B = np.linalg.inv(fX.T.dot(fX)).dot(fX.T)\n",
    "    # reshape data to apply worker function to each gridcell\n",
    "    data_reshape = data.reshape((data.shape[0], data.shape[1] * data.shape[2]))\n",
    "    result_reshape = np.apply_along_axis(seasonality_removal_worker, 0, data_reshape, I = B, X = fX)\n",
    "    result = result_reshape.reshape((data.shape[0], data.shape[1], data.shape[2]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the MJJAS GPH500 for each dataset and each domain\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        for i in range(1979,2023):\n",
    "            GPH500_i = xr.open_dataarray(reanalyses_dir[ds] + 'GPH500_daily_' + ds + '_' + str(i) + '.nc')\n",
    "            MJJAS = GPH500_i.time.dt.month.isin(range(5,10))\n",
    "            GPH500_i = GPH500_i.sel(time=MJJAS)\n",
    "            GPH500_i = sel_domain(GPH500_i,dom)\n",
    "            if i == 1979:\n",
    "                GPH500_all = GPH500_i\n",
    "            else:\n",
    "                GPH500_all = xr.concat([GPH500_all,GPH500_i],dim='time')\n",
    "        if ds == 'era5':\n",
    "            GPH500_all = GPH500_all.drop('expver')\n",
    "        GPH500_all.to_netcdf('data/GPH500_' + ds + '_' + dom + '_' + '1979_2022_MJJAS_raw.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the MJJAS t2m for each dataset and each domain\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        for i in range(1979,2023):\n",
    "            if ds == 'era5':\n",
    "                T2m_i = xr.open_dataarray(reanalyses_dir[ds] + '2m_temperature_daily_' + ds + '_' + str(i) + '.nc')\n",
    "            else:\n",
    "                T2m_i = xr.open_dataset(reanalyses_dir[ds] + 'jra_daily_t2m_' + str(i) + '.nc')['var11']\n",
    "            MJJAS = T2m_i.time.dt.month.isin(range(5,10))\n",
    "            T2m_i = T2m_i.sel(time=MJJAS)\n",
    "            T2m_i = sel_domain(T2m_i,dom)\n",
    "            if i == 1979:\n",
    "                T2m_all = T2m_i\n",
    "            else:\n",
    "                T2m_all = xr.concat([T2m_all,T2m_i],dim='time')\n",
    "        T2m_all.to_netcdf('data/T2m_' + ds + '_' + dom + '_' + '1979_2022_MJJAS_raw.nc')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the GPH500 for each dataset and each domain\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        for i in range(1979,2023):\n",
    "            GPH500_i = xr.open_dataarray(reanalyses_dir[ds] + 'GPH500_daily_' + ds + '_' + str(i) + '.nc')\n",
    "            GPH500_i = GPH500_i.sel(time=~((GPH500_i.time.dt.month == 2) & (GPH500_i.time.dt.day == 29)))\n",
    "            GPH500_i = sel_domain(GPH500_i,dom)\n",
    "            if i == 1979:\n",
    "                GPH500_all = GPH500_i\n",
    "            else:\n",
    "                GPH500_all = xr.concat([GPH500_all,GPH500_i],dim='time')\n",
    "        if ds == 'era5':\n",
    "            GPH500_all = GPH500_all.drop('expver')\n",
    "        GPH500_all.to_netcdf('data/GPH500_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the MJJAS t2m for each dataset and each domain\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        for i in range(1979,2023):\n",
    "            if ds == 'era5':\n",
    "                T2m_i = xr.open_dataarray(reanalyses_dir[ds] + '2m_temperature_daily_' + ds + '_' + str(i) + '.nc')\n",
    "            else:\n",
    "                T2m_i = xr.open_dataset(reanalyses_dir[ds] + 'jra_daily_t2m_' + str(i) + '.nc')['var11']\n",
    "            T2m_i = T2m_i.sel(time=~((T2m_i.time.dt.month == 2) & (T2m_i.time.dt.day == 29)))\n",
    "            T2m_i = sel_domain(T2m_i,dom)\n",
    "            if i == 1979:\n",
    "                T2m_all = T2m_i\n",
    "            else:\n",
    "                T2m_all = xr.concat([T2m_all,T2m_i],dim='time')\n",
    "        T2m_all.to_netcdf('data/T2m_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-pass for raw GPH500\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        GPH500 = xr.open_dataset('data/GPH500_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')\n",
    "        gph_name = {'era5':'z','jra55':'hgt'}\n",
    "        GPH500_ano = GPH500.assign(anom = (('time', 'lat', 'lon'), seasonality_removal_vals(GPH500[gph_name[ds]].values, k=3)))\n",
    "        GPH500_ano = GPH500_ano.anom.sel(time = GPH500_ano.time.dt.month.isin(range(5,10))) ## Select MJJAS\n",
    "        GPH500_ano.to_netcdf('data/GPH500_anomalies_' + ds + '_' + dom + '_' + '1979_2022_MJJAS.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-pass for raw T2m\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        T2m = xr.open_dataset('data/T2m_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')\n",
    "        if ds == 'jra55':\n",
    "            T2m = T2m.squeeze()\n",
    "        t2m_name = {'era5':'t2m','jra55':'var11'}\n",
    "        T2m_ano = T2m.assign(anom = (('time', 'lat', 'lon'), seasonality_removal_vals(T2m[t2m_name[ds]].values, k=3)))\n",
    "        T2m_ano = T2m_ano.anom.sel(time = T2m_ano.time.dt.month.isin(range(5,10))) ## Select MJJAS\n",
    "        T2m_ano.to_netcdf('data/T2m_anomalies_' + ds + '_' + dom + '_' + '1979_2022_MJJAS.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # high-pass for 5-day running average filtered GPH500\n",
    "# for the aim to the reduce synoptic scale fluctuations\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        GPH500 = xr.open_dataset('data/GPH500_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')\n",
    "        gph_name = {'era5':'z','jra55':'hgt'}\n",
    "        GPH500 = GPH500.rolling(time=5, center=False, min_periods=1).mean()\n",
    "        GPH500_ano = GPH500.assign(anom = (('time', 'lat', 'lon'), seasonality_removal_vals(GPH500[gph_name[ds]].values, k=3)))\n",
    "        GPH500_ano = GPH500_ano.anom.sel(time = GPH500_ano.time.dt.month.isin(range(5,10))) ## Select MJJAS\n",
    "        GPH500_ano.to_netcdf('data/GPH500_5day_running_anomalies_' + ds + '_' + dom + '_' + '1979_2022_MJJAS.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-pass for 5-day running average filtered T2m\n",
    "# for the aim to the reduce synoptic scale fluctuations\n",
    "for ds in datasets:\n",
    "    for dom in domains:\n",
    "        T2m = xr.open_dataset('data/T2m_' + ds + '_' + dom + '_' + '1979_2022_raw.nc')\n",
    "        if ds == 'jra55':\n",
    "            T2m = T2m.squeeze()\n",
    "        t2m_name = {'era5':'t2m','jra55':'var11'}\n",
    "        T2m = T2m.rolling(time=5, center=False, min_periods=1).mean()\n",
    "        T2m_ano = T2m.assign(anom = (('time', 'lat', 'lon'), seasonality_removal_vals(T2m[t2m_name[ds]].values, k=3)))\n",
    "        T2m_ano = T2m_ano.anom.sel(time = T2m_ano.time.dt.month.isin(range(5,10))) ## Select MJJAS\n",
    "        T2m_ano.to_netcdf('data/T2m_5day_running_anomalies_' + ds + '_' + dom + '_' + '1979_2022_MJJAS.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate for plot\n",
    "ds = 'era5'\n",
    "for i in range(1979,2023):\n",
    "    GPH500_i = xr.open_dataarray(reanalyses_dir[ds] + 'GPH500_daily_' + ds + '_' + str(i) + '.nc')\n",
    "    MJJAS = GPH500_i.time.dt.month.isin(range(5,10))\n",
    "    GPH500_i = GPH500_i.sel(time=MJJAS)\n",
    "    GPH500_i = GPH500_i.sel(latitude = slice(60,10),longitude = slice(70,160))\n",
    "    if i == 1979:\n",
    "        GPH500_all = GPH500_i\n",
    "    else:\n",
    "        GPH500_all = xr.concat([GPH500_all,GPH500_i],dim='time')\n",
    "if ds == 'era5':\n",
    "    GPH500_all = GPH500_all.drop('expver')\n",
    "\n",
    "gph_name = {'era5':'z','jra55':'hgt'}\n",
    "GPH500_all = GPH500_all.rolling(time=5, center=False, min_periods=1).mean()\n",
    "GPH500_all = GPH500_all.to_dataset()\n",
    "GPH500_ano = GPH500_all.assign(anom = (('time', 'latitude', 'longitude'), seasonality_removal_vals(GPH500_all[gph_name[ds]].values, k=3)))\n",
    "GPH500_ano = GPH500_ano.anom.sel(time = GPH500_ano.time.dt.month.isin(range(5,10))) ## Select MJJAS\n",
    "GPH500_ano.to_netcdf('data/GPH500_5day_running_anomalies_' + ds + '_' + '1979_2022_MJJAS_lat_10_60_lon_70_160.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPH500_all.to_netcdf('data/GPH500_' + ds + '_' + '1979_2022_MJJAS_lat_10_60_lon_70_160.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate for plot\n",
    "ds = 'era5'\n",
    "for i in range(1979,2023):\n",
    "    u_i = xr.open_dataarray(reanalyses_dir[ds] + 'u_component_of_wind_daily_' + ds + '_' + str(i) + '.nc')\n",
    "    u850_i = u_i.sel(level=850)\n",
    "    MJJAS = u850_i.time.dt.month.isin(range(5,10))\n",
    "    u850_i = u850_i.sel(time=MJJAS)\n",
    "    u850_i = u850_i.sel(latitude = slice(60,10),longitude = slice(70,160))\n",
    "    if i == 1979:\n",
    "        u850_all = u850_i\n",
    "    else:\n",
    "        u850_all = xr.concat([u850_all,u850_i],dim='time')\n",
    "if ds == 'era5':\n",
    "    u850_all = u850_all.drop('expver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "u850_1979_2010 = u850_all.sel(time=u850_all.time.dt.year.isin(range(1979,2011)))\n",
    "u850_1979_2010_shape = u850_1979_2010.values.shape\n",
    "u850_1979_2010_reshape = u850_1979_2010.values.reshape((32, int(u850_1979_2010_shape[0] / 32), u850_1979_2010_shape[1], u850_1979_2010_shape[2]))\n",
    "u850_1979_2010_reshape_clim_mean = u850_1979_2010_reshape.mean(axis = 0)\n",
    "u850_ano = u850_all.values.reshape((44, 153, u850_1979_2010_shape[1], u850_1979_2010_shape[2])) - u850_1979_2010_reshape_clim_mean\n",
    "u850_ano = u850_ano.reshape(44*153, 51, 91)\n",
    "u850_ano = xr.DataArray(data=u850_ano, coords=[u850_all.time, u850_all.latitude, u850_all.longitude], dims = ['time','lat','lon'])\n",
    "u850_ano.to_netcdf('data/u850_anomalies_' + ds + '_' + '1979_2022_MJJAS_lat_10_60_lon_70_160.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate for plot\n",
    "ds = 'era5'\n",
    "for i in range(1979,2023):\n",
    "    u_i = xr.open_dataarray(reanalyses_dir[ds] + 'v_component_of_wind_daily_' + ds + '_' + str(i) + '.nc')\n",
    "    v850_i = u_i.sel(level=850)\n",
    "    MJJAS = v850_i.time.dt.month.isin(range(5,10))\n",
    "    v850_i = v850_i.sel(time=MJJAS)\n",
    "    v850_i = v850_i.sel(latitude = slice(60,10),longitude = slice(70,160))\n",
    "    if i == 1979:\n",
    "        v850_all = v850_i\n",
    "    else:\n",
    "        v850_all = xr.concat([v850_all,v850_i],dim='time')\n",
    "if ds == 'era5':\n",
    "    v850_all = v850_all.drop('expver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "v850_1979_2010 = v850_all.sel(time=v850_all.time.dt.year.isin(range(1979,2011)))\n",
    "v850_1979_2010_shape = v850_1979_2010.values.shape\n",
    "v850_1979_2010_reshape = v850_1979_2010.values.reshape((32, int(v850_1979_2010_shape[0] / 32), v850_1979_2010_shape[1], v850_1979_2010_shape[2]))\n",
    "v850_1979_2010_reshape_clim_mean = v850_1979_2010_reshape.mean(axis = 0)\n",
    "v850_ano = v850_all.values.reshape((44, 153, v850_1979_2010_shape[1], v850_1979_2010_shape[2])) - v850_1979_2010_reshape_clim_mean\n",
    "v850_ano = v850_ano.reshape(44*153, 51, 91)\n",
    "v850_ano = xr.DataArray(data=v850_ano, coords=[v850_all.time, v850_all.latitude, v850_all.longitude], dims = ['time','lat','lon'])\n",
    "v850_ano.to_netcdf('data/v850_anomalies_' + ds + '_' + '1979_2022_MJJAS_lat_10_60_lon_70_160.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct circulation analogues for each day in JJA (we mainly focus on summer)\n",
    "The analogues are detected by minimized the distance between the targe day in a specific year and the same calendar day with a 61-day window (30 days before and after the calendar day) in all other years. Four distance functions are applied in this study, mainly, **Euclidean distance**, **Pearson correlation**, **Spearman correlation**, and **Teweles-Wobus skill score**.\n",
    "\n",
    "The potential analogues pool for each target day: total = 61 * (44 - 1) = 2623 days\n",
    "\n",
    "i) From the pool <span style=\"color:red\">(**2623 days**)</span> identify the *Na* closest options (<span style=\"color:red\">**20 days**</span>, similar to Zhuang et al who found that increase the number of analogues could not significant increase the performance);\n",
    "\n",
    "$$S_t=S_aβ+e$$\n",
    "then the temperature anomalies (the dynamical component) for the target day can be estimated:\n",
    "$$T_{dc}=T_sβ$$\n",
    "ii) Repeat *Nr* times (<span style=\"color:red\">**60 times**</span>, 3 domains x 4 distance functions x every 5 days), with the mean of all *Nr* samples as the final dynamical component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA analysis for 2022\n",
    "for ds in datasets:    \n",
    "    for dom in domain_name:\n",
    "        GPH500_ano_mjjas = xr.open_dataarray('data/GPH500_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        t2m_ano_mjjas = xr.open_dataarray('data/t2m_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        all_times = GPH500_ano_mjjas.time\n",
    "        all_years = list(range(1979,2023))\n",
    "        n_year = len(all_years) # each year has 153 days in MJJAS\n",
    "        target_year = 2022\n",
    "        GPH500_ano_mjjas_2022 = GPH500_ano_mjjas.sel(time=GPH500_ano_mjjas.time.dt.year==2022)\n",
    "        GPH500_ano_jja_2022 = GPH500_ano_mjjas_2022.sel(time=GPH500_ano_mjjas_2022.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "        t2m_ano_mjjas_2022 = t2m_ano_mjjas.sel(time=t2m_ano_mjjas.time.dt.year==2022)\n",
    "        t2m_ano_jja_2022 = t2m_ano_mjjas_2022.sel(time=t2m_ano_mjjas_2022.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "        lon = GPH500_ano_jja_2022.lon\n",
    "        lat = GPH500_ano_jja_2022.lat\n",
    "\n",
    "        nlat = len(lat)\n",
    "        nlon = len(lon)\n",
    "        temp_OLS = np.empty((4, 5, 92,nlat*nlon)) # 4 for 4 distance functions, 5 for every other 5 days\n",
    "\n",
    "        for dist_func in dist_funcs:\n",
    "            for i in range(92):       # total 92 days in JJA\n",
    "                GPH500_ano_jja_2022_i = GPH500_ano_jja_2022[i]\n",
    "                pool_i_time = []\n",
    "                # for y in range(len(all_years)):\n",
    "                for y in range(len(all_years)):\n",
    "                    if all_years[y] == target_year: # skip index for the target year\n",
    "                        pass\n",
    "                    else:\n",
    "                        pool_i_time = pool_i_time + list(all_times[(y*153+i):(61+y*153+i)].values) # 24 and 39 represent 24th May and 7th June when i is for 1st June\n",
    "                GPH500_ano_pool_i = GPH500_ano_mjjas[GPH500_ano_mjjas.time.isin(pool_i_time)]\n",
    "                t2m_ano_pool_i = t2m_ano_mjjas[t2m_ano_mjjas.time.isin(pool_i_time)]\n",
    "                if dist_func != 'tws':\n",
    "                    ndays = GPH500_ano_pool_i.shape[0]\n",
    "                    nlat  = GPH500_ano_pool_i.shape[1]\n",
    "                    nlon  = GPH500_ano_pool_i.shape[2]\n",
    "                    GPH500_ano_pool_i = GPH500_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                    GPH500_ano_pool_i_1 = GPH500_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                    GPH500_ano_pool_i_2 = GPH500_ano_pool_i[1::5]\n",
    "                    GPH500_ano_pool_i_3 = GPH500_ano_pool_i[2::5]\n",
    "                    GPH500_ano_pool_i_4 = GPH500_ano_pool_i[3::5]\n",
    "                    GPH500_ano_pool_i_5 = GPH500_ano_pool_i[4::5]\n",
    "                    t2m_ano_pool_i = t2m_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                    t2m_ano_pool_i_1 = t2m_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                    t2m_ano_pool_i_2 = t2m_ano_pool_i[1::5]\n",
    "                    t2m_ano_pool_i_3 = t2m_ano_pool_i[2::5]\n",
    "                    t2m_ano_pool_i_4 = t2m_ano_pool_i[3::5]\n",
    "                    t2m_ano_pool_i_5 = t2m_ano_pool_i[4::5]\n",
    "                    pool_i_time_1 = pool_i_time[0::5]\n",
    "                    pool_i_time_2 = pool_i_time[1::5]\n",
    "                    pool_i_time_3 = pool_i_time[2::5]\n",
    "                    pool_i_time_4 = pool_i_time[3::5]\n",
    "                    pool_i_time_5 = pool_i_time[4::5]\n",
    "                    GPH500_ano_jja_2022_i = GPH500_ano_jja_2022_i.values.reshape((1,nlat*nlon))\n",
    "                    if dist_func == dist_funcs[0]: # Euclidean distance\n",
    "                        dist_pool_1 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1, metric='euclidean')[0]\n",
    "                        dist_pool_2 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2, metric='euclidean')[0]\n",
    "                        dist_pool_3 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3, metric='euclidean')[0]\n",
    "                        dist_pool_4 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4, metric='euclidean')[0]\n",
    "                        dist_pool_5 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5, metric='euclidean')[0]\n",
    "                    elif dist_func == dist_funcs[1]: # Pearson correlation\n",
    "                        dist_pool_1 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]))[0,1:]\n",
    "                        dist_pool_2 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]))[0,1:]\n",
    "                        dist_pool_3 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]))[0,1:]\n",
    "                        dist_pool_4 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]))[0,1:]\n",
    "                        dist_pool_5 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]))[0,1:]\n",
    "                    elif dist_func == dist_funcs[2]: # Spearman correlation\n",
    "                        dist_pool_1 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]),axis=1)[0][0,1:]\n",
    "                        dist_pool_2 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]),axis=1)[0][0,1:]\n",
    "                        dist_pool_3 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]),axis=1)[0][0,1:]\n",
    "                        dist_pool_4 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]),axis=1)[0][0,1:]\n",
    "                        dist_pool_5 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]),axis=1)[0][0,1:]\n",
    "                    elif dist_func == dist_funcs[3]: # Cosine similarity\n",
    "                        dist_pool_1 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1)[0]\n",
    "                        dist_pool_2 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2)[0]\n",
    "                        dist_pool_3 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3)[0]\n",
    "                        dist_pool_4 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4)[0]\n",
    "                        dist_pool_5 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5)[0]\n",
    "                    if dist_func == dist_funcs[0]:\n",
    "                        pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                        pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                        pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                        pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                        pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                        t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                        t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                        t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                        t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                        t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                        pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[:20]]\n",
    "                        pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[:20]]\n",
    "                        pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[:20]]\n",
    "                        pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[:20]]\n",
    "                        pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[:20]]\n",
    "                    else:\n",
    "                        pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                        pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                        pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                        pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                        pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                        t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                        t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                        t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                        t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                        t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[-20:]]\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                pat_pool_best20_da_1 = xr.DataArray(pat_pool_best20_1.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_1,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_2 = xr.DataArray(pat_pool_best20_2.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_2,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_3 = xr.DataArray(pat_pool_best20_3.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_3,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_4 = xr.DataArray(pat_pool_best20_4.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_4,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_5 = xr.DataArray(pat_pool_best20_5.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_5,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "\n",
    "                pat_pool_best20_da_1.to_netcdf('data/similarity_pattern_2022/similarity_pattern_' + ds + '_' + dom + '_' + dist_func + '_one_' + str(i).zfill(2) + '_r.nc')\n",
    "                pat_pool_best20_da_2.to_netcdf('data/similarity_pattern_2022/similarity_pattern_' + ds + '_' + dom + '_' + dist_func + '_two_' + str(i).zfill(2) + '_r.nc')\n",
    "                pat_pool_best20_da_3.to_netcdf('data/similarity_pattern_2022/similarity_pattern_' + ds + '_' + dom + '_' + dist_func + '_three_' + str(i).zfill(2) + '_r.nc')\n",
    "                pat_pool_best20_da_4.to_netcdf('data/similarity_pattern_2022/similarity_pattern_' + ds + '_' + dom + '_' + dist_func + '_four_' + str(i).zfill(2) + '_r.nc')\n",
    "                pat_pool_best20_da_5.to_netcdf('data/similarity_pattern_2022/similarity_pattern_' + ds + '_' + dom + '_' + dist_func + '_five_' + str(i).zfill(2) + '_r.nc')\n",
    "\n",
    "                ## dynamic adjustment\n",
    "                X1 = pat_pool_best20_1.transpose()\n",
    "                X2 = pat_pool_best20_2.transpose()\n",
    "                X3 = pat_pool_best20_3.transpose()\n",
    "                X4 = pat_pool_best20_4.transpose()\n",
    "                X5 = pat_pool_best20_5.transpose()\n",
    "                y = GPH500_ano_jja_2022_i[0]\n",
    "                Xt_1 = t2m_pool_best20_1.transpose()\n",
    "                Xt_2 = t2m_pool_best20_2.transpose()\n",
    "                Xt_3 = t2m_pool_best20_3.transpose()\n",
    "                Xt_4 = t2m_pool_best20_4.transpose()\n",
    "                Xt_5 = t2m_pool_best20_5.transpose()\n",
    "                ######### OLS #########\n",
    "                # Construct temp analog\n",
    "                B1 = np.linalg.lstsq(X1, y, rcond = None)[0]\n",
    "                B2 = np.linalg.lstsq(X2, y, rcond = None)[0]\n",
    "                B3 = np.linalg.lstsq(X3, y, rcond = None)[0]\n",
    "                B4 = np.linalg.lstsq(X4, y, rcond = None)[0]\n",
    "                B5 = np.linalg.lstsq(X5, y, rcond = None)[0]\n",
    "                temp_OLS[dist_funcs.index(dist_func),0,i,:] = np.matmul(Xt_1, B1)\n",
    "                temp_OLS[dist_funcs.index(dist_func),1,i,:] = np.matmul(Xt_2, B2)\n",
    "                temp_OLS[dist_funcs.index(dist_func),2,i,:] = np.matmul(Xt_3, B3)\n",
    "                temp_OLS[dist_funcs.index(dist_func),3,i,:] = np.matmul(Xt_4, B4)\n",
    "                temp_OLS[dist_funcs.index(dist_func),4,i,:] = np.matmul(Xt_5, B5)\n",
    "\n",
    "        temp_OLS_reshape = temp_OLS.reshape(4,5,92,nlat,nlon)\n",
    "        every_other_5day = ['one','two','three','four','five']\n",
    "        temp_OLS_reshape = xr.DataArray(\n",
    "            temp_OLS_reshape,coords=[dist_funcs,every_other_5day,GPH500_ano_jja_2022.time.values,lat,lon],\n",
    "            name='dynamic_t2m',dims=['dist_func','every_other_5day','target_time','lat','lon'])\n",
    "        temp_OLS_reshape.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_r.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era5D1E_Dist\n",
      "era5D1P_Corr\n",
      "era5D1S_Corr\n",
      "era5D1Cosine\n",
      "era5D2E_Dist\n",
      "era5D2P_Corr\n",
      "era5D2S_Corr\n",
      "era5D2Cosine\n",
      "era5D3E_Dist\n",
      "era5D3P_Corr\n",
      "era5D3S_Corr\n",
      "era5D3Cosine\n",
      "jra55D1E_Dist\n",
      "jra55D1P_Corr\n",
      "jra55D1S_Corr\n",
      "jra55D1Cosine\n",
      "jra55D2E_Dist\n",
      "jra55D2P_Corr\n",
      "jra55D2S_Corr\n",
      "jra55D2Cosine\n",
      "jra55D3E_Dist\n",
      "jra55D3P_Corr\n",
      "jra55D3S_Corr\n",
      "jra55D3Cosine\n"
     ]
    }
   ],
   "source": [
    "# CCA analysis for 2022\n",
    "for ds in datasets:    \n",
    "    for dom in domain_name:\n",
    "        GPH500_ano_mjjas = xr.open_dataarray('data/GPH500_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        t2m_ano_mjjas = xr.open_dataarray('data/t2m_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        all_times = GPH500_ano_mjjas.time\n",
    "        all_years = list(range(1979,2023))\n",
    "        n_year = len(all_years) # each year has 153 days in MJJAS\n",
    "        target_year = 2022\n",
    "        GPH500_ano_mjjas_2022 = GPH500_ano_mjjas.sel(time=GPH500_ano_mjjas.time.dt.year==2022)\n",
    "        GPH500_ano_jja_2022 = GPH500_ano_mjjas_2022.sel(time=GPH500_ano_mjjas_2022.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "        t2m_ano_mjjas_2022 = t2m_ano_mjjas.sel(time=t2m_ano_mjjas.time.dt.year==2022)\n",
    "        t2m_ano_jja_2022 = t2m_ano_mjjas_2022.sel(time=t2m_ano_mjjas_2022.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "        lon = GPH500_ano_jja_2022.lon\n",
    "        lat = GPH500_ano_jja_2022.lat\n",
    "\n",
    "        nlat = len(lat)\n",
    "        nlon = len(lon)\n",
    "        temp_OLS = np.empty((4, 5, 92,nlat*nlon)) # 4 for 4 distance functions, 5 for every other 5 days\n",
    "\n",
    "        for dist_func in dist_funcs:\n",
    "            for i in range(92):       # total 92 days in JJA\n",
    "                if i != 85:\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "                GPH500_ano_jja_2022_i = GPH500_ano_jja_2022[i]\n",
    "                t2m_ano_jja_2022_i = t2m_ano_jja_2022[i]\n",
    "                pool_i_time = []\n",
    "                # for y in range(len(all_years)):\n",
    "                for y in range(len(all_years)):\n",
    "                    if all_years[y] == target_year: # skip index for the target year\n",
    "                        pass\n",
    "                    else:\n",
    "                        pool_i_time = pool_i_time + list(all_times[(y*153+i):(61+y*153+i)].values) # 24 and 39 represent 24th May and 7th June when i is for 1st June\n",
    "                GPH500_ano_pool_i = GPH500_ano_mjjas[GPH500_ano_mjjas.time.isin(pool_i_time)]\n",
    "                t2m_ano_pool_i = t2m_ano_mjjas[t2m_ano_mjjas.time.isin(pool_i_time)]\n",
    "                if dist_func != 'tws':\n",
    "                    ndays = GPH500_ano_pool_i.shape[0]\n",
    "                    nlat  = GPH500_ano_pool_i.shape[1]\n",
    "                    nlon  = GPH500_ano_pool_i.shape[2]\n",
    "                    GPH500_ano_pool_i = GPH500_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                    GPH500_ano_pool_i_1 = GPH500_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                    GPH500_ano_pool_i_2 = GPH500_ano_pool_i[1::5]\n",
    "                    GPH500_ano_pool_i_3 = GPH500_ano_pool_i[2::5]\n",
    "                    GPH500_ano_pool_i_4 = GPH500_ano_pool_i[3::5]\n",
    "                    GPH500_ano_pool_i_5 = GPH500_ano_pool_i[4::5]\n",
    "                    t2m_ano_pool_i = t2m_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                    t2m_ano_pool_i_1 = t2m_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                    t2m_ano_pool_i_2 = t2m_ano_pool_i[1::5]\n",
    "                    t2m_ano_pool_i_3 = t2m_ano_pool_i[2::5]\n",
    "                    t2m_ano_pool_i_4 = t2m_ano_pool_i[3::5]\n",
    "                    t2m_ano_pool_i_5 = t2m_ano_pool_i[4::5]\n",
    "                    pool_i_time_1 = pool_i_time[0::5]\n",
    "                    pool_i_time_2 = pool_i_time[1::5]\n",
    "                    pool_i_time_3 = pool_i_time[2::5]\n",
    "                    pool_i_time_4 = pool_i_time[3::5]\n",
    "                    pool_i_time_5 = pool_i_time[4::5]\n",
    "                    GPH500_ano_jja_2022_i = GPH500_ano_jja_2022_i.values.reshape((1,nlat*nlon))\n",
    "                    if dist_func == dist_funcs[0]: # Euclidean distance\n",
    "                        dist_pool_1 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1, metric='euclidean')[0]\n",
    "                        dist_pool_2 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2, metric='euclidean')[0]\n",
    "                        dist_pool_3 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3, metric='euclidean')[0]\n",
    "                        dist_pool_4 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4, metric='euclidean')[0]\n",
    "                        dist_pool_5 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5, metric='euclidean')[0]\n",
    "                    elif dist_func == dist_funcs[1]: # Pearson correlation\n",
    "                        dist_pool_1 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]))[0,1:]\n",
    "                        dist_pool_2 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]))[0,1:]\n",
    "                        dist_pool_3 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]))[0,1:]\n",
    "                        dist_pool_4 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]))[0,1:]\n",
    "                        dist_pool_5 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]))[0,1:]\n",
    "                    elif dist_func == dist_funcs[2]: # Spearman correlation\n",
    "                        dist_pool_1 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]),axis=1)[0][0,1:]\n",
    "                        dist_pool_2 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]),axis=1)[0][0,1:]\n",
    "                        dist_pool_3 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]),axis=1)[0][0,1:]\n",
    "                        dist_pool_4 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]),axis=1)[0][0,1:]\n",
    "                        dist_pool_5 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]),axis=1)[0][0,1:]\n",
    "                    elif dist_func == dist_funcs[3]: # Cosine similarity\n",
    "                        dist_pool_1 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1)[0]\n",
    "                        dist_pool_2 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2)[0]\n",
    "                        dist_pool_3 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3)[0]\n",
    "                        dist_pool_4 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4)[0]\n",
    "                        dist_pool_5 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5)[0]\n",
    "                    if dist_func == dist_funcs[0]:\n",
    "                        pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                        pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                        pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                        pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                        pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                        t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                        t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                        t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                        t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                        t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                        pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[:20]]\n",
    "                        pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[:20]]\n",
    "                        pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[:20]]\n",
    "                        pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[:20]]\n",
    "                        pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[:20]]\n",
    "                    else:\n",
    "                        pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                        pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                        pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                        pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                        pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                        t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                        t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                        t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                        t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                        t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[-20:]]\n",
    "                        pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[-20:]]\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                ## dynamic adjustment\n",
    "                X1 = pat_pool_best20_1.transpose()\n",
    "                X2 = pat_pool_best20_2.transpose()\n",
    "                X3 = pat_pool_best20_3.transpose()\n",
    "                X4 = pat_pool_best20_4.transpose()\n",
    "                X5 = pat_pool_best20_5.transpose()\n",
    "                y = GPH500_ano_jja_2022_i[0]\n",
    "                Xt_1 = t2m_pool_best20_1.transpose()\n",
    "                Xt_2 = t2m_pool_best20_2.transpose()\n",
    "                Xt_3 = t2m_pool_best20_3.transpose()\n",
    "                Xt_4 = t2m_pool_best20_4.transpose()\n",
    "                Xt_5 = t2m_pool_best20_5.transpose()\n",
    "                ######### OLS #########\n",
    "                # Construct temp analog\n",
    "                B1 = np.linalg.lstsq(X1, y, rcond = None)[0]\n",
    "                B2 = np.linalg.lstsq(X2, y, rcond = None)[0]\n",
    "                B3 = np.linalg.lstsq(X3, y, rcond = None)[0]\n",
    "                B4 = np.linalg.lstsq(X4, y, rcond = None)[0]\n",
    "                B5 = np.linalg.lstsq(X5, y, rcond = None)[0]\n",
    "                temp_OLS[dist_funcs.index(dist_func),0,i,:] = np.matmul(Xt_1, B1)\n",
    "                temp_OLS[dist_funcs.index(dist_func),1,i,:] = np.matmul(Xt_2, B2)\n",
    "                temp_OLS[dist_funcs.index(dist_func),2,i,:] = np.matmul(Xt_3, B3)\n",
    "                temp_OLS[dist_funcs.index(dist_func),3,i,:] = np.matmul(Xt_4, B4)\n",
    "                temp_OLS[dist_funcs.index(dist_func),4,i,:] = np.matmul(Xt_5, B5)\n",
    "\n",
    "                if i == 85:\n",
    "                    y_reshape_1 = xr.DataArray(np.matmul(X1, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_pattern',dims=['lat','lon'])\n",
    "                    y_reshape_2 = xr.DataArray(np.matmul(X2, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_pattern',dims=['lat','lon'])\n",
    "                    y_reshape_3 = xr.DataArray(np.matmul(X3, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_pattern',dims=['lat','lon'])\n",
    "                    y_reshape_4 = xr.DataArray(np.matmul(X4, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_pattern',dims=['lat','lon'])\n",
    "                    y_reshape_5 = xr.DataArray(np.matmul(X5, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_pattern',dims=['lat','lon'])\n",
    "                    y_original = xr.DataArray(y.reshape(nlat,nlon),coords=[lat,lon],name='original_circulation_pattern',dims=['lat','lon'])\n",
    "                    temp_OLS_1 = xr.DataArray(np.matmul(Xt_1, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_dynamic_t2m',dims=['lat','lon'])\n",
    "                    temp_OLS_2 = xr.DataArray(np.matmul(Xt_2, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_dynamic_t2m',dims=['lat','lon'])\n",
    "                    temp_OLS_3 = xr.DataArray(np.matmul(Xt_3, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_dynamic_t2m',dims=['lat','lon'])\n",
    "                    temp_OLS_4 = xr.DataArray(np.matmul(Xt_4, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_dynamic_t2m',dims=['lat','lon'])\n",
    "                    temp_OLS_5 = xr.DataArray(np.matmul(Xt_5, B1).reshape(nlat,nlon),coords=[lat,lon],name='CCA_constructed_dynamic_t2m',dims=['lat','lon'])\n",
    "                    x_t2m_original = xr.DataArray(t2m_ano_jja_2022_i,coords=[lat,lon],name='original_t2m',dims=['lat','lon'])\n",
    "                    y_reshape_1.to_netcdf('data/dynamic_t2m_2022/constructed_pattern_' + ds + '_' + dom + '_' + dist_func + '_one_' + str(i).zfill(2) + '_r.nc')\n",
    "                    y_reshape_2.to_netcdf('data/dynamic_t2m_2022/constructed_pattern_' + ds + '_' + dom + '_' + dist_func + '_two_' + str(i).zfill(2) + '_r.nc')\n",
    "                    y_reshape_3.to_netcdf('data/dynamic_t2m_2022/constructed_pattern_' + ds + '_' + dom + '_' + dist_func + '_three_' + str(i).zfill(2) + '_r.nc')\n",
    "                    y_reshape_4.to_netcdf('data/dynamic_t2m_2022/constructed_pattern_' + ds + '_' + dom + '_' + dist_func + '_four_' + str(i).zfill(2) + '_r.nc')\n",
    "                    y_reshape_5.to_netcdf('data/dynamic_t2m_2022/constructed_pattern_' + ds + '_' + dom + '_' + dist_func + '_five_' + str(i).zfill(2) + '_r.nc')\n",
    "                    y_original.to_netcdf('data/dynamic_t2m_2022/original_pattern_' + ds + '_' + dom + '_' + dist_func + str(i).zfill(2) + '_r.nc')\n",
    "                    temp_OLS_1.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_' + dist_func + '_one_' + str(i).zfill(2) + '_r.nc')\n",
    "                    temp_OLS_2.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_' + dist_func + '_two_' + str(i).zfill(2) + '_r.nc')\n",
    "                    temp_OLS_3.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_' + dist_func + '_three_' + str(i).zfill(2) + '_r.nc')\n",
    "                    temp_OLS_4.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_' + dist_func + '_four_' + str(i).zfill(2) + '_r.nc')\n",
    "                    temp_OLS_5.to_netcdf('data/dynamic_t2m_2022/dynamic_t2m_' + ds + '_' + dom + '_' + dist_func + '_five_' + str(i).zfill(2) + '_r.nc')\n",
    "                    x_t2m_original.to_netcdf('data/dynamic_t2m_2022/original_t2m_' + ds + '_' + dom + '_' + dist_func + str(i).zfill(2) + '_r.nc')\n",
    "\n",
    "                pat_pool_best20_da_1 = xr.DataArray(t2m_pool_best20_1.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_1,lat,lon],name='Dynamic adjusted T2m',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_2 = xr.DataArray(t2m_pool_best20_2.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_2,lat,lon],name='Dynamic adjusted T2m',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_3 = xr.DataArray(t2m_pool_best20_3.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_3,lat,lon],name='Dynamic adjusted T2m',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_4 = xr.DataArray(t2m_pool_best20_4.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_4,lat,lon],name='Dynamic adjusted T2m',dims=['analogue_time','lat','lon'])\n",
    "                pat_pool_best20_da_5 = xr.DataArray(t2m_pool_best20_5.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_5,lat,lon],name='Dynamic adjusted T2m',dims=['analogue_time','lat','lon'])\n",
    "\n",
    "            print(ds + dom + dist_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPH500_ano_mjjas = xr.open_dataarray('data/GPH500_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "t2m_ano_mjjas = xr.open_dataarray('data/t2m_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "all_times = GPH500_ano_mjjas.time\n",
    "all_years = list(range(1979,2023))\n",
    "n_year = len(all_years) # each year has 153 days in MJJAS\n",
    "target_year = 2022\n",
    "GPH500_ano_mjjas_2022 = GPH500_ano_mjjas.sel(time=GPH500_ano_mjjas.time.dt.year==2022)\n",
    "GPH500_ano_jja_2022 = GPH500_ano_mjjas_2022.sel(time=GPH500_ano_mjjas_2022.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "lon = GPH500_ano_jja_2022.lon\n",
    "lat = GPH500_ano_jja_2022.lat\n",
    "nlat = len(lat)\n",
    "nlon = len(lon)\n",
    "dist_func = dist_funcs[0]\n",
    "\n",
    "temp_OLS_1 = np.empty((92,nlat*nlon))\n",
    "temp_OLS_2 = np.empty((92,nlat*nlon))\n",
    "temp_OLS_3 = np.empty((92,nlat*nlon))\n",
    "temp_OLS_4 = np.empty((92,nlat*nlon))\n",
    "temp_OLS_5 = np.empty((92,nlat*nlon))\n",
    "\n",
    "for i in range(92):       # total 92 days in JJA\n",
    "    GPH500_ano_jja_2022_i = GPH500_ano_jja_2022[i]\n",
    "    t2m_ano_jja_2022_i = t2m_ano_jja_2022[i]\n",
    "    pool_i_time = []\n",
    "    # for y in range(len(all_years)):\n",
    "    for y in range(len(all_years)):\n",
    "        if all_years[y] == target_year: # skip index for the target year\n",
    "            pass\n",
    "        else:\n",
    "            pool_i_time = pool_i_time + list(all_times[(y*153+i):(61+y*153+i)].values) # 24 and 39 represent 24th May and 7th June when i is for 1st June\n",
    "    GPH500_ano_pool_i = GPH500_ano_mjjas[GPH500_ano_mjjas.time.isin(pool_i_time)]\n",
    "    t2m_ano_pool_i = t2m_ano_mjjas[t2m_ano_mjjas.time.isin(pool_i_time)]\n",
    "    if dist_func != 'tws':\n",
    "        ndays = GPH500_ano_pool_i.shape[0]\n",
    "        nlat  = GPH500_ano_pool_i.shape[1]\n",
    "        nlon  = GPH500_ano_pool_i.shape[2]\n",
    "        GPH500_ano_pool_i = GPH500_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "        GPH500_ano_pool_i_1 = GPH500_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "        GPH500_ano_pool_i_2 = GPH500_ano_pool_i[1::5]\n",
    "        GPH500_ano_pool_i_3 = GPH500_ano_pool_i[2::5]\n",
    "        GPH500_ano_pool_i_4 = GPH500_ano_pool_i[3::5]\n",
    "        GPH500_ano_pool_i_5 = GPH500_ano_pool_i[4::5]\n",
    "        t2m_ano_pool_i = t2m_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "        t2m_ano_pool_i_1 = t2m_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "        t2m_ano_pool_i_2 = t2m_ano_pool_i[1::5]\n",
    "        t2m_ano_pool_i_3 = t2m_ano_pool_i[2::5]\n",
    "        t2m_ano_pool_i_4 = t2m_ano_pool_i[3::5]\n",
    "        t2m_ano_pool_i_5 = t2m_ano_pool_i[4::5]\n",
    "        pool_i_time_1 = pool_i_time[0::5]\n",
    "        pool_i_time_2 = pool_i_time[1::5]\n",
    "        pool_i_time_3 = pool_i_time[2::5]\n",
    "        pool_i_time_4 = pool_i_time[3::5]\n",
    "        pool_i_time_5 = pool_i_time[4::5]\n",
    "        GPH500_ano_jja_2022_i = GPH500_ano_jja_2022_i.values.reshape((1,nlat*nlon))\n",
    "        t2m_ano_jja_2022_i = t2m_ano_jja_2022_i.values.reshape((1,nlat*nlon))\n",
    "        if dist_func == dist_funcs[0]: # Euclidean distance\n",
    "            dist_pool_1 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1, metric='euclidean')[0]\n",
    "            dist_pool_2 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2, metric='euclidean')[0]\n",
    "            dist_pool_3 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3, metric='euclidean')[0]\n",
    "            dist_pool_4 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4, metric='euclidean')[0]\n",
    "            dist_pool_5 = distance.cdist(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5, metric='euclidean')[0]\n",
    "        elif dist_func == dist_funcs[1]: # Pearson correlation\n",
    "            dist_pool_1 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]))[0,1:]\n",
    "            dist_pool_2 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]))[0,1:]\n",
    "            dist_pool_3 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]))[0,1:]\n",
    "            dist_pool_4 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]))[0,1:]\n",
    "            dist_pool_5 = np.corrcoef(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]))[0,1:]\n",
    "        elif dist_func == dist_funcs[2]: # Spearman correlation\n",
    "            dist_pool_1 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_1]),axis=1)[0][0,1:]\n",
    "            dist_pool_2 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_2]),axis=1)[0][0,1:]\n",
    "            dist_pool_3 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_3]),axis=1)[0][0,1:]\n",
    "            dist_pool_4 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_4]),axis=1)[0][0,1:]\n",
    "            dist_pool_5 = spearmanr(np.row_stack([GPH500_ano_jja_2022_i,GPH500_ano_pool_i_5]),axis=1)[0][0,1:]\n",
    "        elif dist_func == dist_funcs[3]: # Cosine similarity\n",
    "            dist_pool_1 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_1)[0]\n",
    "            dist_pool_2 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_2)[0]\n",
    "            dist_pool_3 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_3)[0]\n",
    "            dist_pool_4 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_4)[0]\n",
    "            dist_pool_5 = cosine_similarity(GPH500_ano_jja_2022_i, GPH500_ano_pool_i_5)[0]\n",
    "        if dist_func == dist_funcs[0]:\n",
    "            pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "            pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "            pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "            pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "            pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "            t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "            t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "            t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "            t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "            t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "            pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[:20]]\n",
    "            pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[:20]]\n",
    "            pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[:20]]\n",
    "            pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[:20]]\n",
    "            pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[:20]]\n",
    "        else:\n",
    "            pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "            pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "            pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "            pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "            pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "            t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "            t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "            t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "            t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "            t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "            pat_pool_best20_time_1 = np.array(pool_i_time_1)[dist_pool_1.argsort()[-20:]]\n",
    "            pat_pool_best20_time_2 = np.array(pool_i_time_2)[dist_pool_2.argsort()[-20:]]\n",
    "            pat_pool_best20_time_3 = np.array(pool_i_time_3)[dist_pool_3.argsort()[-20:]]\n",
    "            pat_pool_best20_time_4 = np.array(pool_i_time_4)[dist_pool_4.argsort()[-20:]]\n",
    "            pat_pool_best20_time_5 = np.array(pool_i_time_5)[dist_pool_5.argsort()[-20:]]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    pat_pool_best20_da_1 = xr.DataArray(pat_pool_best20_1.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_1,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "    pat_pool_best20_da_2 = xr.DataArray(pat_pool_best20_2.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_2,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "    pat_pool_best20_da_3 = xr.DataArray(pat_pool_best20_3.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_3,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "    pat_pool_best20_da_4 = xr.DataArray(pat_pool_best20_4.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_4,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "    pat_pool_best20_da_5 = xr.DataArray(pat_pool_best20_5.reshape(20,nlat,nlon),coords=[pat_pool_best20_time_5,lat,lon],name='Analogue circulation pattern',dims=['analogue_time','lat','lon'])\n",
    "\n",
    "    ## dynamic adjustment\n",
    "    X1 = pat_pool_best20_1.transpose()\n",
    "    X2 = pat_pool_best20_2.transpose()\n",
    "    X3 = pat_pool_best20_3.transpose()\n",
    "    X4 = pat_pool_best20_4.transpose()\n",
    "    X5 = pat_pool_best20_5.transpose()\n",
    "    y = GPH500_ano_jja_2022_i[0]\n",
    "    Xt_1 = t2m_pool_best20_1.transpose()\n",
    "    Xt_2 = t2m_pool_best20_2.transpose()\n",
    "    Xt_3 = t2m_pool_best20_3.transpose()\n",
    "    Xt_4 = t2m_pool_best20_4.transpose()\n",
    "    Xt_5 = t2m_pool_best20_5.transpose()\n",
    "    ######### OLS #########\n",
    "    # Construct temp analog\n",
    "    B1 = np.linalg.lstsq(X1, y, rcond = None)[0]\n",
    "    B2 = np.linalg.lstsq(X2, y, rcond = None)[0]\n",
    "    B3 = np.linalg.lstsq(X3, y, rcond = None)[0]\n",
    "    B4 = np.linalg.lstsq(X4, y, rcond = None)[0]\n",
    "    B5 = np.linalg.lstsq(X5, y, rcond = None)[0]\n",
    "    temp_OLS_1[i,:] = np.matmul(Xt_1, B1)\n",
    "    temp_OLS_2[i,:] = np.matmul(Xt_2, B2)\n",
    "    temp_OLS_3[i,:] = np.matmul(Xt_3, B3)\n",
    "    temp_OLS_4[i,:] = np.matmul(Xt_4, B4)\n",
    "    temp_OLS_5[i,:] = np.matmul(Xt_5, B5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCA analysis for historical period\n",
    "for ds in datasets:    \n",
    "    for dom in domain_name:\n",
    "        GPH500_ano_mjjas = xr.open_dataarray('data/GPH500_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        t2m_ano_mjjas = xr.open_dataarray('data/t2m_5day_running_anomalies_' + ds + '_' + dom + '_1979_2022_MJJAS.nc')\n",
    "        all_times = GPH500_ano_mjjas.time\n",
    "        all_years = list(range(1979,2023))\n",
    "        n_year = len(all_years) # each year has 153 days in MJJAS\n",
    "        \n",
    "        lon = GPH500_ano_mjjas.lon\n",
    "        lat = GPH500_ano_mjjas.lat\n",
    "        nlat = len(lat)\n",
    "        nlon = len(lon)\n",
    "        temp_OLS = np.empty((44, 4, 5, 92,nlat*nlon)) # 44 for 44 years in the historical period, 4 for 4 distance functions, 5 for every other 5 days\n",
    "\n",
    "        for ty in range(1979,2023):\n",
    "            target_year = ty\n",
    "            GPH500_ano_mjjas_ty = GPH500_ano_mjjas.sel(time=GPH500_ano_mjjas.time.dt.year==ty)\n",
    "            GPH500_ano_jja_ty = GPH500_ano_mjjas_ty.sel(time=GPH500_ano_mjjas_ty.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "            t2m_ano_mjjas_ty = t2m_ano_mjjas.sel(time=t2m_ano_mjjas.time.dt.year==ty)\n",
    "            t2m_ano_jja_ty = t2m_ano_mjjas_ty.sel(time=t2m_ano_mjjas_ty.time.dt.month.isin(range(6,9))) # focus on JJA\n",
    "\n",
    "            for dist_func in dist_funcs:\n",
    "                for i in range(92):       # total 92 days in JJA\n",
    "                    GPH500_ano_jja_ty_i = GPH500_ano_jja_ty[i]\n",
    "                    pool_i_time = []\n",
    "                    # for y in range(len(all_years)):\n",
    "                    for y in range(len(all_years)):\n",
    "                        if all_years[y] == target_year: # skip index for the target year\n",
    "                            pass\n",
    "                        else:\n",
    "                            pool_i_time = pool_i_time + list(all_times[(y*153+i):(61+y*153+i)].values) # 24 and 39 represent 24th May and 7th June when i is for 1st June\n",
    "                    GPH500_ano_pool_i = GPH500_ano_mjjas[GPH500_ano_mjjas.time.isin(pool_i_time)]\n",
    "                    t2m_ano_pool_i = t2m_ano_mjjas[t2m_ano_mjjas.time.isin(pool_i_time)]\n",
    "                    if dist_func != 'tws':\n",
    "                        ndays = GPH500_ano_pool_i.shape[0]\n",
    "                        nlat  = GPH500_ano_pool_i.shape[1]\n",
    "                        nlon  = GPH500_ano_pool_i.shape[2]\n",
    "                        GPH500_ano_pool_i = GPH500_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                        GPH500_ano_pool_i_1 = GPH500_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                        GPH500_ano_pool_i_2 = GPH500_ano_pool_i[1::5]\n",
    "                        GPH500_ano_pool_i_3 = GPH500_ano_pool_i[2::5]\n",
    "                        GPH500_ano_pool_i_4 = GPH500_ano_pool_i[3::5]\n",
    "                        GPH500_ano_pool_i_5 = GPH500_ano_pool_i[4::5]\n",
    "                        t2m_ano_pool_i = t2m_ano_pool_i.values.reshape((ndays,nlat*nlon))\n",
    "                        t2m_ano_pool_i_1 = t2m_ano_pool_i[0::5]  # select every other 5 days to avoid selecting from consecutive days from the same weather event\n",
    "                        t2m_ano_pool_i_2 = t2m_ano_pool_i[1::5]\n",
    "                        t2m_ano_pool_i_3 = t2m_ano_pool_i[2::5]\n",
    "                        t2m_ano_pool_i_4 = t2m_ano_pool_i[3::5]\n",
    "                        t2m_ano_pool_i_5 = t2m_ano_pool_i[4::5]\n",
    "                        pool_i_time_1 = pool_i_time[0::5]\n",
    "                        pool_i_time_2 = pool_i_time[1::5]\n",
    "                        pool_i_time_3 = pool_i_time[2::5]\n",
    "                        pool_i_time_4 = pool_i_time[3::5]\n",
    "                        pool_i_time_5 = pool_i_time[4::5]\n",
    "                        GPH500_ano_jja_ty_i = GPH500_ano_jja_ty_i.values.reshape((1,nlat*nlon))\n",
    "                        if dist_func == dist_funcs[0]: # Euclidean distance\n",
    "                            dist_pool_1 = distance.cdist(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_1, metric='euclidean')[0]\n",
    "                            dist_pool_2 = distance.cdist(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_2, metric='euclidean')[0]\n",
    "                            dist_pool_3 = distance.cdist(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_3, metric='euclidean')[0]\n",
    "                            dist_pool_4 = distance.cdist(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_4, metric='euclidean')[0]\n",
    "                            dist_pool_5 = distance.cdist(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_5, metric='euclidean')[0]\n",
    "                        elif dist_func == dist_funcs[1]: # Pearson correlation\n",
    "                            dist_pool_1 = np.corrcoef(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_1]))[0,1:]\n",
    "                            dist_pool_2 = np.corrcoef(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_2]))[0,1:]\n",
    "                            dist_pool_3 = np.corrcoef(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_3]))[0,1:]\n",
    "                            dist_pool_4 = np.corrcoef(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_4]))[0,1:]\n",
    "                            dist_pool_5 = np.corrcoef(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_5]))[0,1:]\n",
    "                        elif dist_func == dist_funcs[2]: # Spearman correlation\n",
    "                            dist_pool_1 = spearmanr(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_1]),axis=1)[0][0,1:]\n",
    "                            dist_pool_2 = spearmanr(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_2]),axis=1)[0][0,1:]\n",
    "                            dist_pool_3 = spearmanr(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_3]),axis=1)[0][0,1:]\n",
    "                            dist_pool_4 = spearmanr(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_4]),axis=1)[0][0,1:]\n",
    "                            dist_pool_5 = spearmanr(np.row_stack([GPH500_ano_jja_ty_i,GPH500_ano_pool_i_5]),axis=1)[0][0,1:]\n",
    "                        elif dist_func == dist_funcs[3]: # Cosine similarity\n",
    "                            dist_pool_1 = cosine_similarity(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_1)[0]\n",
    "                            dist_pool_2 = cosine_similarity(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_2)[0]\n",
    "                            dist_pool_3 = cosine_similarity(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_3)[0]\n",
    "                            dist_pool_4 = cosine_similarity(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_4)[0]\n",
    "                            dist_pool_5 = cosine_similarity(GPH500_ano_jja_ty_i, GPH500_ano_pool_i_5)[0]\n",
    "                        if dist_func == dist_funcs[0]:\n",
    "                            pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                            pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                            pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                            pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                            pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                            t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[:20]]\n",
    "                            t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[:20]]\n",
    "                            t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[:20]]\n",
    "                            t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[:20]]\n",
    "                            t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[:20]]\n",
    "                        else:\n",
    "                            pat_pool_best20_1 = GPH500_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                            pat_pool_best20_2 = GPH500_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                            pat_pool_best20_3 = GPH500_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                            pat_pool_best20_4 = GPH500_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                            pat_pool_best20_5 = GPH500_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                            t2m_pool_best20_1 = t2m_ano_pool_i_1[dist_pool_1.argsort()[-20:]]\n",
    "                            t2m_pool_best20_2 = t2m_ano_pool_i_2[dist_pool_2.argsort()[-20:]]\n",
    "                            t2m_pool_best20_3 = t2m_ano_pool_i_3[dist_pool_3.argsort()[-20:]]\n",
    "                            t2m_pool_best20_4 = t2m_ano_pool_i_4[dist_pool_4.argsort()[-20:]]\n",
    "                            t2m_pool_best20_5 = t2m_ano_pool_i_5[dist_pool_5.argsort()[-20:]]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    ## dynamic adjustment\n",
    "                    X1 = pat_pool_best20_1.transpose()\n",
    "                    X2 = pat_pool_best20_2.transpose()\n",
    "                    X3 = pat_pool_best20_3.transpose()\n",
    "                    X4 = pat_pool_best20_4.transpose()\n",
    "                    X5 = pat_pool_best20_5.transpose()\n",
    "                    y = GPH500_ano_jja_ty_i[0]\n",
    "                    Xt_1 = t2m_pool_best20_1.transpose()\n",
    "                    Xt_2 = t2m_pool_best20_2.transpose()\n",
    "                    Xt_3 = t2m_pool_best20_3.transpose()\n",
    "                    Xt_4 = t2m_pool_best20_4.transpose()\n",
    "                    Xt_5 = t2m_pool_best20_5.transpose()\n",
    "                    ######### OLS #########\n",
    "                    # Construct temp analog\n",
    "                    B1 = np.linalg.lstsq(X1, y, rcond = None)[0]\n",
    "                    B2 = np.linalg.lstsq(X2, y, rcond = None)[0]\n",
    "                    B3 = np.linalg.lstsq(X3, y, rcond = None)[0]\n",
    "                    B4 = np.linalg.lstsq(X4, y, rcond = None)[0]\n",
    "                    B5 = np.linalg.lstsq(X5, y, rcond = None)[0]\n",
    "                    temp_OLS[range(1979,2023).index(ty),dist_funcs.index(dist_func),0,i,:] = np.matmul(Xt_1, B1)\n",
    "                    temp_OLS[range(1979,2023).index(ty),dist_funcs.index(dist_func),1,i,:] = np.matmul(Xt_2, B2)\n",
    "                    temp_OLS[range(1979,2023).index(ty),dist_funcs.index(dist_func),2,i,:] = np.matmul(Xt_3, B3)\n",
    "                    temp_OLS[range(1979,2023).index(ty),dist_funcs.index(dist_func),3,i,:] = np.matmul(Xt_4, B4)\n",
    "                    temp_OLS[range(1979,2023).index(ty),dist_funcs.index(dist_func),4,i,:] = np.matmul(Xt_5, B5)\n",
    "\n",
    "        temp_OLS_reshape = temp_OLS.reshape(44,4,5,92,nlat,nlon)\n",
    "        every_other_5day = ['one','two','three','four','five']\n",
    "        year = range(1979,2023)\n",
    "        temp_OLS_reshape = xr.DataArray(\n",
    "            temp_OLS_reshape,coords=[year,dist_funcs,every_other_5day,GPH500_ano_jja_ty.time.values,lat,lon],\n",
    "            name='dynamic_t2m',dims=['year','dist_func','every_other_5day','target_time','lat','lon'])\n",
    "        temp_OLS_reshape.to_netcdf('data/dynamic_t2m_historical/dynamic_t2m_' + ds + '_' + dom + '.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfMac38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85633838223fa24745b04ee2425a29866e58f21a05e261f135ffa2db214a5274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
